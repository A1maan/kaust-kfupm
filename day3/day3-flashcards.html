<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Learning Flashcards</title>
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Lucide Icons -->
    <script src="https://unpkg.com/lucide@latest"></script>
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>

    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f8fafc; /* Slate 50 */
        }

        .math-font {
            font-family: 'JetBrains Mono', monospace;
        }

        /* 3D Flip Effects */
        .perspective-1000 {
            perspective: 1000px;
        }

        .preserve-3d {
            transform-style: preserve-3d;
        }

        .backface-hidden {
            backface-visibility: hidden;
            -webkit-backface-visibility: hidden;
        }

        .rotate-y-180 {
            transform: rotateY(180deg);
        }

        .card-inner {
            transition: transform 0.6s cubic-bezier(0.4, 0, 0.2, 1);
        }

        .is-flipped {
            transform: rotateY(180deg);
        }

        /* Animations */
        @keyframes slideIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .animate-slide-in {
            animation: slideIn 0.3s ease-out forwards;
        }

        /* Progress Bar Transition */
        .progress-transition {
            transition: width 0.3s ease-in-out;
        }
    </style>
</head>
<body class="min-h-screen flex flex-col items-center py-8 px-4 text-slate-800">

    <div id="app" class="w-full max-w-4xl flex flex-col items-center">

        <!-- Header -->
        <div class="w-full flex flex-col md:flex-row justify-between items-center mb-8 gap-4">
            <div>
                <h1 class="text-3xl font-bold text-slate-900 tracking-tight">Deep Learning Flashcards</h1>
                <p class="text-slate-500 font-medium mt-1">KAUST Academy - Day 3 Concepts - Prepared by Safwan</p>
            </div>

            <div class="flex items-center gap-4">
                <div class="bg-white px-4 py-2 rounded-lg shadow-sm border border-slate-200 text-sm font-semibold text-slate-600">
                    <span id="current-card-num">1</span> / <span id="total-cards-num">45</span>
                </div>
                <button onclick="shuffleCards()" class="p-2 text-slate-500 hover:text-blue-600 hover:bg-blue-50 rounded-full transition-colors" title="Shuffle Cards">
                    <i data-lucide="shuffle" class="w-5 h-5"></i>
                </button>
            </div>
        </div>

        <!-- Progress Bar -->
        <div class="w-full h-1.5 bg-slate-200 rounded-full mb-8 overflow-hidden">
            <div id="progress-bar" class="h-full bg-blue-600 rounded-full progress-transition" style="width: 0%"></div>
        </div>

        <!-- Card Container -->
        <div class="perspective-1000 w-full aspect-[4/3] md:aspect-[2/1] min-h-[400px] mb-8 group cursor-pointer" onclick="flipCard()">
            <div id="card-inner" class="card-inner relative w-full h-full preserve-3d shadow-xl rounded-2xl">

                <!-- Front Side (Question) -->
                <div class="absolute w-full h-full backface-hidden bg-white rounded-2xl border-2 border-slate-100 flex flex-col items-center justify-center p-8 md:p-12 text-center select-none">
                    <div class="absolute top-6 left-6">
                        <span class="bg-blue-100 text-blue-800 text-xs font-bold px-3 py-1 rounded-full uppercase tracking-wider">Question</span>
                    </div>
                    <div class="w-16 h-16 bg-blue-50 rounded-full flex items-center justify-center mb-6 text-blue-600">
                        <i data-lucide="help-circle" class="w-8 h-8"></i>
                    </div>
                    <h2 id="card-front-text" class="text-2xl md:text-3xl font-bold text-slate-800 leading-snug">
                        <!-- Content Injected via JS -->
                    </h2>
                    <p class="absolute bottom-6 text-slate-400 text-sm font-medium flex items-center animate-pulse">
                        <i data-lucide="mouse-pointer-2" class="w-4 h-4 mr-2"></i> Click to flip
                    </p>
                </div>

                <!-- Back Side (Answer) -->
                <div class="absolute w-full h-full backface-hidden rotate-y-180 bg-slate-900 rounded-2xl flex flex-col items-center justify-center p-8 md:p-12 text-center select-none overflow-y-auto">
                    <div class="absolute top-6 left-6">
                        <span class="bg-emerald-500/20 text-emerald-400 text-xs font-bold px-3 py-1 rounded-full uppercase tracking-wider border border-emerald-500/20">Answer</span>
                    </div>
                    <div id="card-back-content" class="text-xl md:text-2xl font-medium text-slate-100 leading-relaxed">
                        <!-- Content Injected via JS -->
                    </div>
                </div>

            </div>
        </div>

        <!-- Controls -->
        <div class="flex items-center gap-4 md:gap-6">
            <button onclick="prevCard()" class="flex items-center justify-center w-14 h-14 rounded-full bg-white border border-slate-200 text-slate-700 shadow-sm hover:bg-slate-50 hover:border-slate-300 hover:shadow-md transition-all active:scale-95 disabled:opacity-50 disabled:cursor-not-allowed" id="prev-btn">
                <i data-lucide="arrow-left" class="w-6 h-6"></i>
            </button>

            <button onclick="flipCard()" class="flex items-center gap-2 px-8 py-4 bg-blue-600 text-white rounded-xl font-bold shadow-lg shadow-blue-200 hover:bg-blue-700 hover:shadow-xl transition-all active:scale-95 min-w-[160px] justify-center">
                <i data-lucide="rotate-cw" class="w-5 h-5"></i>
                <span id="flip-btn-text">Reveal Answer</span>
            </button>

            <button onclick="nextCard()" class="flex items-center justify-center w-14 h-14 rounded-full bg-white border border-slate-200 text-slate-700 shadow-sm hover:bg-slate-50 hover:border-slate-300 hover:shadow-md transition-all active:scale-95 disabled:opacity-50 disabled:cursor-not-allowed" id="next-btn">
                <i data-lucide="arrow-right" class="w-6 h-6"></i>
            </button>
        </div>

        <!-- Keyboard Hint -->
        <div class="mt-8 text-slate-400 text-xs font-medium flex gap-4">
            <span class="flex items-center"><kbd class="bg-white border border-slate-200 px-2 py-0.5 rounded mr-1.5 font-sans">Space</kbd> Flip</span>
            <span class="flex items-center"><kbd class="bg-white border border-slate-200 px-2 py-0.5 rounded mr-1.5 font-sans">Left</kbd> Prev</span>
            <span class="flex items-center"><kbd class="bg-white border border-slate-200 px-2 py-0.5 rounded mr-1.5 font-sans">Right</kbd> Next</span>
        </div>

    </div>

    <script>
        // --- Data ---
        const initialCards = [
            // Introduction and Fundamentals
            {
                q: "What is Deep Learning?",
                a: "Machine Learning powered by Artificial Neural Networks with many layers. \'Neural\' is inspired by brain neurons; \'Deep\' refers to the number of layers."
            },
            {
                q: "How does Deep Learning scale differently than traditional ML?",
                a: "Traditional ML hits a performance ceiling as data increases. Deep Learning keeps improving with more data."
            },
            {
                q: "State the Universal Approximation Theorem.",
                a: "A neural network with enough neurons can approximate any continuous function to arbitrary accuracy."
            },
            // Network Architecture
            {
                q: "What are the three essential parts of a neural network?",
                a: "(1) Input layer receives raw data. (2) Hidden layers extract patterns and features. (3) Output layer produces the final prediction."
            },
            {
                q: "What two operations does a single neuron perform?",
                a: "(1) Linear combination: $z = w^T x + b$. (2) Non-linear activation: $a = \\sigma(z)$."
            },
            {
                q: "Why are activation functions necessary?",
                a: "Without activations, stacking layers creates another linear function (Linear + Linear = Linear). Activations add non-linearity to fit complex patterns."
            },
            {
                q: "Compare Sigmoid, Tanh, and ReLU activations.",
                a: "Sigmoid: outputs $(0, 1)$, common in logistic regression. Tanh: outputs $(-1, 1)$, zero-centered. ReLU: $\\max(0, z)$, common for deep networks."
            },
            {
                q: "How do we represent a full layer mathematically?",
                a: "$Z = W x + b$ (vectorized form). Each row of $W$ contains one neuron\'s weights, computing all neurons in parallel."
            },
            {
                q: "How do we determine the number of neurons in each layer?",
                a: "Input layer matches the number of features. Output layer matches the task (1 for regression, $C$ for $C$-class classification). Hidden layers are a design choice."
            },
            {
                q: "What network shapes are used for different tasks?",
                a: "Regression: output 1 with MSE loss. Binary classification: output 1 with sigmoid and binary cross-entropy. Multiclass: output $C$ with softmax and cross-entropy."
            },
            // Forward Pass
            {
                q: "What is a forward pass?",
                a: "Compute network output from input. Example: $a^{[0]} = x$, $z^{[1]} = W^{[1]} a^{[0]} + b^{[1]}$, $a^{[1]} = \\sigma(z^{[1]})$, ..., $\\hat{y} = a^{[L]}$."
            },
            {
                q: "What happens after the forward pass?",
                a: "Compute the loss $J(\\theta)$ comparing prediction $\\hat{y}$ with true target $y$, then run backpropagation to compute gradients."
            },
            // Backpropagation
            {
                q: "What is backpropagation?",
                a: "Using the chain rule to calculate gradients of the loss with respect to all parameters by flowing backward through the network."
            },
            {
                q: "Write the chain rule for computing $\\frac{\\partial J}{\\partial w}$ in a neuron.",
                a: "$\\frac{\\partial J}{\\partial w} = \\frac{\\partial J}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$."
            },
            {
                q: "For sigmoid activation, what is $\\frac{\\partial J}{\\partial w}$?",
                a: "If $z = wx + b$, $a = \\sigma(z)$, and $J = \\frac{1}{2}(a - y)^2$, then $\\frac{\\partial J}{\\partial w} = (a - y) \\cdot a(1 - a) \\cdot x$."
            },
            {
                q: "Why is it called backpropagation?",
                a: "Gradients are calculated starting from the loss and propagated backward through each layer to the beginning."
            },
            // Training Process
            {
                q: "What is the basic gradient descent update rule?",
                a: "$\\theta_{new} = \\theta_{old} - \\alpha \\nabla_{\\theta} J(\\theta)$, where $\\alpha$ is the learning rate."
            },
            {
                q: "What is a training step (iteration)?",
                a: "One forward pass, loss calculation, backward pass, and parameter update on a single batch."
            },
            {
                q: "What is a batch in training?",
                a: "A small subset of training samples (for example, 32, 64, or 128) used for one parameter update."
            },
            {
                q: "What is an epoch?",
                a: "One complete pass through the entire training dataset, with all batches processed once."
            },
            {
                q: "How many steps are in one epoch?",
                a: "Steps per epoch $= \\lceil N / B \\rceil$, where $N$ is total samples and $B$ is batch size."
            },
            {
                q: "Why shuffle the dataset between epochs?",
                a: "Shuffling randomizes batch composition to prevent biased gradient estimates and improve generalization."
            },
            {
                q: "Compare Batch GD vs Mini-Batch GD vs Stochastic GD.",
                a: "Batch: uses all data (stable, slow, memory-intensive). Mini-Batch: uses small batches (best balance, GPU-efficient). Stochastic: uses 1 sample (very noisy, slow)."
            },
            // Optimizers - Basic
            {
                q: "What problem does Momentum solve?",
                a: "Vanilla gradient descent oscillates in ravines and makes slow progress. Momentum accumulates velocity to smooth the trajectory and accelerate convergence."
            },
            {
                q: "Write the Momentum update rule.",
                a: "$v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla J(\\theta)$, then $\\theta_{t+1} = \\theta_t - \\alpha v_t$. Typically $\\beta = 0.9$."
            },
            {
                q: "What does AdaGrad do?",
                a: "Gives each parameter its own adaptive learning rate based on gradient history: $\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{G_t} + \\epsilon} g_t$."
            },
            {
                q: "What is the problem with AdaGrad?",
                a: "$G_t$ accumulates all past squared gradients, so the learning rate shrinks toward zero and training stalls."
            },
            // Optimizers - Advanced
            {
                q: "How does RMSProp fix AdaGrad?",
                a: "Uses an exponentially decaying average: $E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta) g_t^2$ (often $\\beta \approx 0.9$)."
            },
            {
                q: "What does Adam combine?",
                a: "Momentum (smooth trajectory) + adaptive learning rates (RMSProp-style) + bias correction."
            },
            {
                q: "What are the two moving averages in Adam?",
                a: "$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$ and $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$."
            },
            {
                q: "Why does Adam need bias correction?",
                a: "Moving averages start at zero, biasing early estimates. Corrections: $\\hat{m}_t = m_t/(1 - \\beta_1^t)$ and $\\hat{v}_t = v_t/(1 - \\beta_2^t)$."
            },
            {
                q: "Write the Adam update rule.",
                a: "$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v}}_t + \\epsilon} \\hat{m}_t$."
            },
            {
                q: "What is the problem with Adam\'s weight decay?",
                a: "L2 regularization is added to the gradient before adaptive scaling, which can distort the intended regularization effect."
            },
            {
                q: "How does AdamW fix this?",
                a: "Decouples weight decay from the gradient update: $\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v}}_t + \\epsilon} \\hat{m}_t - \\alpha \\lambda \\theta_t$."
            },
            {
                q: "What optimizer should you use by default?",
                a: "AdamW. It is robust and works well across many problems in practice."
            },
            // Design Decisions
            {
                q: "What are the key hyperparameters to set?",
                a: "Learning rate $\\alpha$, batch size, number of epochs, momentum coefficients $\\beta_1$ and $\\beta_2$, and weight decay $\\lambda$."
            },
            {
                q: "What architectural decisions must you make?",
                a: "Number of layers, neurons per layer, activation functions, output layer shape and activation, and network type (MLP/CNN/RNN)."
            },
            {
                q: "What are the three types of neural network architectures?",
                a: "MLP (Multi-Layer Perceptron) for tabular data, CNN (Convolutional) for images, RNN (Recurrent) for sequences/time series."
            },
            // Context and Memory
            {
                q: "Why do we not compute gradients on the entire dataset?",
                a: "It is computationally expensive, requires massive memory, and slows updates. Mini-batches provide frequent updates and good gradient estimates."
            },
            {
                q: "How does a neural network \"remember\" patterns?",
                a: "Through learned weights (parameters). Training adjusts weights via gradient descent to encode patterns from data."
            },
            {
                q: "What is the relationship between neurons, layers, and networks?",
                a: "Neuron = basic unit. Layer = collection of neurons in parallel. Network = stack of layers with data flowing through them."
            },
            {
                q: "Why is the loss function critical?",
                a: "It is the objective we optimize. It defines good vs bad predictions and varies by task (MSE for regression, cross-entropy for classification)."
            },
            {
                q: "What is the difference between parameters and hyperparameters?",
                a: "Parameters are learned during training (weights, biases). Hyperparameters are set before training (learning rate, batch size, layers)."
            }
        ];

        // --- State ---
        let cards = [...initialCards];
        let currentIndex = 0;
        let isFlipped = false;

        // --- DOM Elements ---
        const cardInner = document.getElementById('card-inner');
        const cardFrontText = document.getElementById('card-front-text');
        const cardBackContent = document.getElementById('card-back-content');
        const currentNumEl = document.getElementById('current-card-num');
        const totalNumEl = document.getElementById('total-cards-num');
        const progressBar = document.getElementById('progress-bar');
        const prevBtn = document.getElementById('prev-btn');
        const nextBtn = document.getElementById('next-btn');
        const flipBtnText = document.getElementById('flip-btn-text');

        // --- Functions ---

        function init() {
            renderCard();
            updateMeta();
            lucide.createIcons();

            // Render Math on load
            renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false}
                ]
            });

            // Keyboard navigation
            document.addEventListener('keydown', (e) => {
                if (e.code === 'Space') {
                    e.preventDefault();
                    flipCard();
                } else if (e.code === 'ArrowRight') {
                    nextCard();
                } else if (e.code === 'ArrowLeft') {
                    prevCard();
                }
            });
        }

        function renderCard() {
            // Reset state
            isFlipped = false;
            cardInner.classList.remove('is-flipped');
            flipBtnText.innerText = "Reveal Answer";

            // Update content with small animation delay to hide swap
            setTimeout(() => {
                const currentCard = cards[currentIndex];
                cardFrontText.innerHTML = currentCard.q;
                cardBackContent.innerHTML = currentCard.a;

                // Re-render math
                renderMathInElement(cardFrontText, {
                     delimiters: [{left: '$', right: '$', display: false}]
                });
                renderMathInElement(cardBackContent, {
                     delimiters: [{left: '$', right: '$', display: false}]
                });
            }, 150);
        }

        function updateMeta() {
            currentNumEl.innerText = currentIndex + 1;
            totalNumEl.innerText = cards.length;

            const progress = ((currentIndex + 1) / cards.length) * 100;
            progressBar.style.width = `${progress}%`;

            prevBtn.disabled = currentIndex === 0;
            nextBtn.disabled = currentIndex === cards.length - 1;
        }

        function flipCard() {
            isFlipped = !isFlipped;
            cardInner.classList.toggle('is-flipped');
            flipBtnText.innerText = isFlipped ? "Show Question" : "Reveal Answer";
        }

        function nextCard() {
            if (currentIndex < cards.length - 1) {
                currentIndex++;
                renderCard();
                updateMeta();
            }
        }

        function prevCard() {
            if (currentIndex > 0) {
                currentIndex--;
                renderCard();
                updateMeta();
            }
        }

        function shuffleCards() {
            // Fisher-Yates shuffle
            for (let i = cards.length - 1; i > 0; i--) {
                const j = Math.floor(Math.random() * (i + 1));
                [cards[i], cards[j]] = [cards[j], cards[i]];
            }
            currentIndex = 0;
            renderCard();
            updateMeta();
        }

        // --- Start ---
        init();

    </script>
</body>
</html>
